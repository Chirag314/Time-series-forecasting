{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " time series practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcYeK6uhKVNItqT88INgXG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag314/Time-series-forecasting/blob/main/time_series_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate multistep time series.\n",
        "Reference : https://github.com/mikepang98/TimeSeriesLSTM/blob/main/Multivariate_Multi_Step_Medium.ipynb\n"
      ],
      "metadata": {
        "id": "Az9FE34Y9-aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from pandas import DataFrame, concat\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "from numpy import mean, concatenate\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Activation\n",
        "from numpy import array ,hstack\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "S6F_Q3XS-JvN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide time series data into 7 days window and 7 days predictions\n",
        "#first convert into numpy array\n",
        "data_train= np.array(data_train)\n",
        "\n",
        "x_train, y_train=[] ,[]\n",
        "\n",
        "for i in (7, len(data_train)-7):\n",
        "  x_train.append(data_train[i-7:i])\n",
        "  y_train.append(data_train[i:i+7])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "dxJ8Bwuy_J6R",
        "outputId": "c7229978-e090-4212-c502-fd183f70b417"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-07c8c9c4b1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Divide time series data into 7 days window and 7 days predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#first convert into numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize data\n",
        "x_scaler= MinMaxScaler()\n",
        "x_train=x_scaler.fit_transform(x_train)\n",
        "y_scaler=MinMaxScaler()\n",
        "y_train = y_scaler.fit_transform(y_train)"
      ],
      "metadata": {
        "id": "g3cpqaaJNwen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM expects three dimension shape.\n",
        "x_train.reshape(x_train[0],x_train[1],1)\n",
        "#x_train.reshape(1098,7,1)"
      ],
      "metadata": {
        "id": "y1bjhh_POalz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "reg = Sequential()\n",
        "reg.add(LSTM(units=200,activation='relu',input_shape=(7,1))\n",
        "reg.add(Dense(7))\n",
        "\n",
        "#compile the model\n",
        "reg.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "#Fit the model\n",
        "reg.fit(x_train, y_train, epochs=100)\n"
      ],
      "metadata": {
        "id": "Gmd6zKFxPG0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the test data\n",
        "x_test, y_test =[][]\n",
        "for i in range(7, len(test_data)-7):\n",
        "  x_test.append(data_test[i-7:i])\n",
        "  y_test.append(data_test[i:i+7]\n",
        "\n",
        "#First convert test data into numpy array          \n",
        "x_test, y_test = np.array(x_test), np.array(y_test)\n",
        "\n",
        "#Normalize test data\n",
        "# We cant use fit_transorm method here as its already used before and it will overfit the test data\n",
        "x_test =x_scaler.transform(x_test)\n",
        "y_test= y_scaler.transform(y_test)\n",
        "\n",
        "\n",
        "#Reshape data so that it can be fed in to LSTM\n",
        "x_test = x_test.reshape(331,7,1)\n",
        "\n",
        "#Predict the test data \n",
        "y_pred = reg.predict(x_test)\n",
        "# Inverse transform the predicted data to original form(de-normalize)\n",
        "y_pred = y_scaler.inverse_transform(y_pred)\n",
        "\n",
        "# Inverse transform the test data to original form(de-normalize)\n",
        "y_true= y_scaler.inverse_transform(y_test)\n",
        "\n",
        "# Evaluate the model\n",
        "\n"
      ],
      "metadata": {
        "id": "LjWCxE4GP_wW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}